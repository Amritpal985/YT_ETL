# ğŸš€ YT_ETL â€“ YouTube ELT Pipeline with Apache Airflow

An end-to-end **ELT data pipeline** that extracts YouTube video data using the YouTube Data API, processes it through Apache Airflow, loads it into a PostgreSQL data warehouse (staging + core layers), and performs automated data quality validation using Soda.

---

## ğŸ“Œ Project Overview

This project demonstrates a production-style data engineering workflow:

1. Extract video & playlist metadata from YouTube API
2. Store raw data as JSON
3. Load data into a PostgreSQL data warehouse
4. Apply layered modeling (staging â†’ core)
5. Run automated data quality checks
6. Fully orchestrated using Apache Airflow

The pipeline runs automatically on a scheduled basis.

---

## ğŸ—ï¸ Architecture

```
YouTube API
     â†“
Airflow DAG 1 (produce_json)
     â†“
Raw JSON File (data/)
     â†“
Airflow DAG 2 (update_db)
     â†“
PostgreSQL (staging schema)
     â†“
PostgreSQL (core schema)
     â†“
Airflow DAG 3 (data_quality)
     â†“
Soda Data Quality Validation
```

---

## âš™ï¸ Workflow Details

### ğŸ”¹ DAG 1 â€“ `produce_json`

* Scheduled daily
* Fetches playlist ID
* Extracts video IDs
* Retrieves video statistics
* Saves raw JSON to local `data/` folder
* Triggers `update_db` DAG

### ğŸ”¹ DAG 2 â€“ `update_db`

* Loads JSON into staging schema
* Transforms and loads into core schema
* Triggers `data_quality` DAG

### ğŸ”¹ DAG 3 â€“ `data_quality`

* Runs Soda checks on:

  * staging schema
  * core schema
* Validates schema consistency and data integrity

---

## ğŸ—„ï¸ Data Warehouse Design

The project follows a **layered architecture**:

* **Staging Schema**

  * Raw structured data from JSON
  * Minimal transformation

* **Core Schema**

  * Cleaned and analytics-ready tables
  * Structured for reporting & BI use

---

## ğŸ› ï¸ Tech Stack

* Python
* Apache Airflow
* PostgreSQL
* Soda (Data Quality Framework)
* Docker
* YouTube Data API

---

## â° Scheduling

* Timezone-aware scheduling
* DAG run timeout handling
* Controlled concurrency
* Cross-DAG triggering using `TriggerDagRunOperator`

---

## ğŸ“‚ Project Structure

```
YT_ETL/
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ main_dag.py
â”‚
â”œâ”€â”€ api/
â”‚   â””â”€â”€ video_stats.py
â”‚
â”œâ”€â”€ datawarehouse/
â”‚   â””â”€â”€ dwh.py
â”‚
â”œâ”€â”€ dataquality/
â”‚   â””â”€â”€ soda.py
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ YT_data_<date>.json
â”‚
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

---

## â–¶ï¸ How to Run

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/Amritpal985/YT_ETL.git
cd YT_ETL
```

### 2ï¸âƒ£ Start Airflow (Docker)

```bash
docker-compose up --build
```

### 3ï¸âƒ£ Open Airflow UI

```
http://localhost:8080
```

Unpause DAGs and wait for scheduled execution.

---

## ğŸ“Š Key Features

* Modular DAG design
* Event-driven DAG triggering
* Staging â†’ Core data modeling
* Automated data quality validation
* Production-style scheduling
* Containerized deployment




